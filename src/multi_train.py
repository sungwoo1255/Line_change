import torch
import torch.optim as optim
import torch.nn as nn
import os
import numpy as np
import wandb

# DDP ì§€ì›ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
import torch.distributed as dist
from torch.utils.data import DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

from data_processing.dataset import NGSIMDataset
from models.model_1 import DualViewInteractionAwareModel
from torch_geometric.data import Data, Batch

# --- Hyperparameters & Configuration ---
# BATCH_SIZEëŠ” ì´ì œ GPUë‹¹ ë°°ì¹˜ í¬ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
# ì´ ë°°ì¹˜ í¬ê¸° = BATCH_SIZE * GPU ìˆ˜
BATCH_SIZE = 128
EPOCHS = 500
LEARNING_RATE = 1e-5
HISTORY_FRAMES = 40
FUTURE_FRAMES = 20
FEATURE_DIM = 4
LSTM_HIDDEN = 64
NUM_HEADS = 8
NUM_CLASSES = 3

# --- DDP ì„¤ì • í•¨ìˆ˜ ---
def setup_ddp():
    """DDP í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì´ˆê¸°í™”í•˜ê³  í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ìˆœìœ„ì™€ ì¥ì¹˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    device_id = rank % torch.cuda.device_count()
    torch.cuda.set_device(device_id)
    return rank, f'cuda:{device_id}'

def cleanup_ddp():
    """DDP í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    dist.destroy_process_group()

def collate_fn(batch_list):
    """
    ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±ëœ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ë¥¼ PyGì˜ 'Batch' ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    DDP í™˜ê²½ì—ì„œë„ ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ì ìœ¼ë¡œ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
    """
    data_list = []
    for item in batch_list:
        data = Data(
            x=item['historical_data'],
            future_x=item['future_data'],
            edge_index=item['edge_index'],
            edge_attr=item['edge_attr'],
            y=item['label'],
            num_nodes=item['historical_data'].shape[0]
        )
        data_list.append(data)
    return Batch.from_data_list(data_list)

def train():
    # --- 1. DDP ë° ì¥ì¹˜ ì„¤ì • ---
    rank, DEVICE = setup_ddp()
    is_main_process = (rank == 0)

    if is_main_process:
        print("--- Starting Model Training with DDP ---")
    
    # ìŠ¤í¬ë¦½íŠ¸ ë° ë°ì´í„° ê²½ë¡œ ì„¤ì •
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    DATA_PATH = os.path.join(SCRIPT_DIR, "..", "data", "NGSIM", "processed_tensors")

    # --- 2. ë°ì´í„° ë¡œë“œ ---
    try:
        full_dataset = NGSIMDataset(data_dir=DATA_PATH)
    except FileNotFoundError as e:
        if is_main_process:
            print(f"Error loading data: {e}")
            print(f"Please ensure the processed data exists at {DATA_PATH}")
        return

    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

    # --- DDPë¥¼ ìœ„í•œ DistributedSampler ì„¤ì • ---
    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, shuffle=False)

    # DataLoaderì— samplerë¥¼ ì „ë‹¬í•˜ê³ , shuffle=Falseë¡œ ì„¤ì • (samplerê°€ ì…”í”Œë§ ë‹´ë‹¹)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=4, pin_memory=True, sampler=train_sampler)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=4, pin_memory=True, sampler=val_sampler)

    if is_main_process:
        print(f"Data loaded. Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}")

    # --- 3. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ---
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ê³„ì‚°í•˜ê³  ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì— ì „íŒŒ
    if is_main_process:
        print("Calculating class weights for weighted loss...")
        train_labels = [sample['label'].item() for sample in train_dataset]
        class_counts = np.bincount(train_labels, minlength=NUM_CLASSES)
        total_samples = float(sum(class_counts))
        class_weights = [total_samples / c if c > 0 else 0 for c in class_counts]
        # GPU ë””ë°”ì´ìŠ¤ë¡œ ë°”ë¡œ í…ì„œ ì´ë™
        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)
    else:
        # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë“¤ì€ ê°€ì¤‘ì¹˜ë¥¼ ë°›ì„ ë¹ˆ í…ì„œë¥¼ í•´ë‹¹ GPU ë””ë°”ì´ìŠ¤ì— ìƒì„±
        class_weights_tensor = torch.empty(NUM_CLASSES, dtype=torch.float32).to(DEVICE)

    # ì´ì œ ëª¨ë“  í…ì„œê°€ GPUì— ìˆìœ¼ë¯€ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤í–‰
    dist.broadcast(class_weights_tensor, src=0)
    
    if is_main_process:
        print(f"Calculated class weights: {class_weights_tensor.cpu().numpy()}")

    # --- 4. ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™” ---
    model = DualViewInteractionAwareModel(
        feature_dim=FEATURE_DIM,
        lstm_hidden=LSTM_HIDDEN,
        num_heads=NUM_HEADS,
        num_classes=NUM_CLASSES
    ).to(DEVICE)

    # DDPë¡œ ëª¨ë¸ ê°ì‹¸ê¸°
    model = DDP(model, device_ids=[int(DEVICE.split(':')[-1])])

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)

    if is_main_process:
        print("Model, Optimizer, and Weighted Loss function initialized for DDP.")
        wandb.init(project="lane-change-prediction", config={
            "learning_rate": LEARNING_RATE, "batch_size": BATCH_SIZE * dist.get_world_size(), "epochs": EPOCHS,
            "architecture": "DDP"
        })

    # --- 5. í•™ìŠµ ë£¨í”„ ---
    best_val_loss = float('inf')
    epochs_no_improvement = 0
    EARLY_STOPPING_PATIENCE = 30

    checkpoint_dir = os.path.join(SCRIPT_DIR, "..", "checkpoints")
    if is_main_process:
        os.makedirs(checkpoint_dir, exist_ok=True)
        save_path = os.path.join(checkpoint_dir, "best_model.pt")
        print(f"Model will be saved to {save_path}")

    for epoch in range(EPOCHS):
        # DistributedSamplerê°€ ë§¤ ì—í¬í¬ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì…”í”Œë§í•˜ë„ë¡ ì„¤ì •
        train_sampler.set_epoch(epoch)
        
        model.train()
        total_train_loss = 0.0
        for batch in train_loader:
            batch = batch.to(DEVICE)
            optimizer.zero_grad()
            # DDP ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ .moduleì„ í†µí•´ ì›ë˜ ëª¨ë¸ì˜ forwardë¥¼ í˜¸ì¶œ
            outputs = model(batch)
            loss = criterion(outputs, batch.y)
            loss.backward() # DDPê°€ ì—¬ê¸°ì„œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë™ê¸°í™”
            optimizer.step()
            total_train_loss += loss.item()

        # --- ê²€ì¦ ë£¨í”„ ---
        model.eval()
        total_val_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(DEVICE)
                outputs = model(batch)
                loss = criterion(outputs, batch.y)
                total_val_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total_samples += batch.y.size(0)
                correct_predictions += (predicted == batch.y).sum().item()

        # ê° í”„ë¡œì„¸ìŠ¤ì˜ ê²°ê³¼ë¥¼ ì§‘ê³„
        # í…ì„œë¡œ ë³€í™˜í•˜ì—¬ all_reduce ì‚¬ìš©
        metrics = torch.tensor([total_train_loss, total_val_loss, correct_predictions, total_samples]).to(DEVICE)
        dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
        
        avg_train_loss = metrics[0] / len(train_loader.dataset)
        avg_val_loss = metrics[1] / len(val_loader.dataset)
        val_accuracy = (metrics[2] / metrics[3]) * 100

        # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ë¡œê·¸ ì¶œë ¥ ë° ëª¨ë¸ ì €ì¥
        if is_main_process:
            print(f"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%")
            wandb.log({"train_loss": avg_train_loss, "val_loss": avg_val_loss, "val_accuracy": val_accuracy})

            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                epochs_no_improvement = 0
                print(f"New best validation loss: {best_val_loss:.4f}. Saving model... ğŸ’¾")
                # DDPì—ì„œëŠ” model.module.state_dict()ë¥¼ ì €ì¥
                torch.save(model.module.state_dict(), save_path)
                wandb.run.summary["best_val_loss"] = best_val_loss
                wandb.run.summary["best_epoch"] = epoch + 1
            else:
                epochs_no_improvement += 1
                print(f"Validation loss did not improve. Patience: {epochs_no_improvement}/{EARLY_STOPPING_PATIENCE}")

            if epochs_no_improvement >= EARLY_STOPPING_PATIENCE:
                print(f"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs without improvement.")
                break
    
    # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    dist.barrier()
    if epochs_no_improvement >= EARLY_STOPPING_PATIENCE and is_main_process:
         # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì—ë„ ì¢…ë£Œ ì‹ í˜¸ë¥¼ ë³´ë‚´ê¸° ìœ„í•´ ì—í¬í¬ ë£¨í”„ë¥¼ ì¤‘ë‹¨
         # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ë£¨í”„ë¥¼ ë¹ ì ¸ë‚˜ì™€ cleanup_ddp()ë¥¼ í˜¸ì¶œí•˜ë„ë¡ í•¨
         pass


    cleanup_ddp()
    if is_main_process:
        print("--- Model Training Complete ---")

if __name__ == "__main__":
    train()